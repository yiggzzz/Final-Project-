{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790871dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file\n",
    "df=pd.read_csv('archive/dataset_phishing.csv')\n",
    "\n",
    "# Show all columns \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edbd63-bea7-4b61-be11-af1c83304f0f",
   "metadata": {},
   "source": [
    "## Before we begin exploring the data, let's quickly analyze some important factors of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af84b00-224b-4d91-a737-84efbb0e8e0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's label legitimate = 1 and phishing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3473a-9678-4e9f-82ce-a87e002f5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run above code to run this cell\n",
    "# Encoding 'status' as label 1 & 0, naming the field as target\n",
    "df['status'] = df['status'].str.contains('legitimate').astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f901ef-93f8-4721-b6c6-bb7f17790260",
   "metadata": {},
   "source": [
    "#### The following code will verify that there are exactly 50% phishing URLs and 50% legitimate URLs - it will separate only the status and URL in one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908ad99-a97f-4948-9324-725c4719f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_df = df[['url','status']]\n",
    "function_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ca858-74ef-45f6-bf27-b85757a66213",
   "metadata": {},
   "source": [
    "### We save this CSV to be our function.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c97ff-8ce6-420a-84d4-919a4b4a3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_df.to_csv(\"archive/function.csv\",index=False,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977e19b-d2ee-4c58-b490-0cba330c03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d945e6-1e3e-4e88-842f-8e76c17c795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "legitimate_list = list(df[df.status == 1].url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cdc2f-8269-4fbf-b57f-05f56fcc95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_list = list(df[df.status == 0].url) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bae505-ec0b-41cc-803f-24d4a8783550",
   "metadata": {},
   "source": [
    "### We create a Dataframe below that separates the Phishing URLs and puts them in one column; then puts all the Legitimate URLs and puts them in another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cafe2-ccdd-4d5f-9496-568d80e6dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Phishing':phishing_list, 'Legitimate':legitimate_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb32be-3321-42a8-b0fe-9cdecf418778",
   "metadata": {},
   "source": [
    "### We are dropping the 'url' column so all of our data are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16e88e-62ac-4e82-92dd-197390c45c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop url column\n",
    "df = df.drop(columns = ['url'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf880f-00a0-47c7-a5d1-9af844cce22b",
   "metadata": {},
   "source": [
    "## Save the dataset for ML use, comes later in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98941fdb-bfaf-419b-b1a5-3fe6528416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"archive/dataset_phishing_functionapplied.csv\",index=False,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25d337",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e53679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows and columns\n",
    "print(\"Number of rows: \",df.shape[0]) \n",
    "print(\"Number of columns: \", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the first 5 rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fa4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting info about columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for null values\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba3b81",
   "metadata": {},
   "source": [
    "### Observation: The dataset has no missing values. All columns are numeric and either integers or float. Result: No cleaning of missing data needed and data types look correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb4327",
   "metadata": {},
   "source": [
    "### Looking for Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate columns\n",
    "\n",
    "duplicates = df.duplicated().sort_values(ascending=False)\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713182eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get duplicate rows\n",
    "\n",
    "duplicates = df.duplicated()\n",
    "df[duplicates]\n",
    "print(\"Number of duplicated rows: \", df[duplicates].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing duplicates values \n",
    "\n",
    "df[duplicates].to_excel(r'archive/duplicates.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all duplicated values\n",
    "\n",
    "cleaned_data= df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1610081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking again if any duplicate values still exist.\n",
    "check = cleaned_data.duplicated()\n",
    "cleaned_data[check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b9eb6",
   "metadata": {},
   "source": [
    "### Observation: The dataset contained 174 duplicated rows. All duplicates were removed from the source dataset. The new dataframe without duplicate values is called \"cleaned_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1746a",
   "metadata": {},
   "source": [
    "### Exploring summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show statistics \n",
    "\n",
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abb8d8",
   "metadata": {},
   "source": [
    "### Observation: \n",
    "length_url, length_hostname, nb_dots, nb_hyphens, nb_and, nb_eq,nb_underscore,nb_percent, nb_slash, nb_semicolumn, nb_space, length_words_raw, char_repeat, shortest_words_raw, shortest_word_host, shortest_word_path,\tlongest_words_raw, longest_word_host, longest_word_path, avg_words_raw,\tavg_word_host, avg_word_pat, phish_hints, np_hyperlinks, nb_extCSS, domain_registration_length, page_rank,have high max_values relative to the percentiles. \n",
    "domain_registration_length and domain_age have negative values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e0e67",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faff9b7",
   "metadata": {},
   "source": [
    "#### Analyzing length_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outliers with a Boxplot\n",
    "\n",
    "URL_length = cleaned_data['length_url']\n",
    "sns.boxplot(data=URL_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c03cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing distribution with a Violinplot\n",
    "\n",
    "URL_length = cleaned_data['length_url']\n",
    "sns.violinplot(data=URL_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of length_url\n",
    "\n",
    "\n",
    "sns.histplot(data=cleaned_data['length_url'],bins=20)\n",
    "plt.title('Distribution of length_url')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding outliers\n",
    "\n",
    "from scipy.stats import iqr\n",
    "iqr = iqr(cleaned_data['length_url'])\n",
    "lower = np.quantile(cleaned_data['length_url'], 0.25) - 1.5 * iqr\n",
    "upper = np.quantile(cleaned_data['length_url'], 0.75) + 1.5 * iqr\n",
    "\n",
    "\n",
    "print(lower)\n",
    "print(upper)\n",
    "\n",
    "#showing outliers (3 in total)\n",
    "outliers= cleaned_data[((cleaned_data['length_url'] < lower)) | (cleaned_data['length_url'] > upper)]\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ', outliers.shape[0], 'outliers for length_url in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cf9fa",
   "metadata": {},
   "source": [
    "#### Observation: Some URLs have a length over 1000.  The histogram shows that the vast majority of values are in the low end of length. Based on a statistical formula, all values above a length of 128 are considered outliers. Further discussion needed to keep them or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7f5c9",
   "metadata": {},
   "source": [
    "#### Analyzing length hostname:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outliers with a Boxplot\n",
    "\n",
    "hostname_length = cleaned_data['length_hostname']\n",
    "sns.boxplot(data= hostname_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of length_hostname\n",
    "\n",
    "\n",
    "sns.histplot(data=cleaned_data['length_hostname'],bins=20)\n",
    "plt.title('Distribution of length_hostname')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f4e52",
   "metadata": {},
   "source": [
    "#### Analyzing  nb_dots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outliers with a Boxplot\n",
    "\n",
    "hostname_length = cleaned_data['nb_dots']\n",
    "sns.boxplot(data= hostname_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of nb_dots\n",
    "\n",
    "sns.histplot(data=cleaned_data['nb_dots'],bins=20)\n",
    "plt.title('Distribution of nb_dots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a627ab",
   "metadata": {},
   "source": [
    "#### Analyzing longest_words_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outliers with a Boxplot\n",
    "\n",
    "hostname_length = cleaned_data['longest_words_raw']\n",
    "sns.boxplot(data= hostname_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ce348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of  longest_words_raw\n",
    "\n",
    "sns.histplot(data=cleaned_data['longest_words_raw'],bins=20)\n",
    "plt.title('Distribution of longest_words_raw')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b20fac",
   "metadata": {},
   "source": [
    "#### Observation: As for length_url there are also some rows with outliers for longest_words_raw, nb_dots and length_hostname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting example of outliers for url_length to Excel\n",
    "# outliers.to_excel(r'outliers_url_length.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a72d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting cleaned data to Excel\n",
    "\n",
    "# cleaned_data.to_csv(r'phishing_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147485ea-a119-4bc4-9919-bb393d6a8708",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c8352-9dc3-42da-b5d3-26d773a60d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57a117-116d-4c2a-8f4a-92b89430bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our input dataset\n",
    "original_df = pd.read_csv('archive/dataset_phishing_functionapplied.csv')\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e6929-2849-49cf-a99a-b1a9bfd11571",
   "metadata": {},
   "source": [
    "#### Let's checkout the columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ebcc6-418d-4d38-96cc-0e39118c9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe1ca6-c19b-40fc-bae8-b7df1f98ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all data types to make sure all data types are numeric and no categorical values are in the dataset\n",
    "original_df.dtypes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e730c3d-3596-4e20-a4dc-39f5d17e81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again that all columns are numeric\n",
    "original_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fb4a0-4eda-4b78-a2ab-d4025f780134",
   "metadata": {},
   "source": [
    "### Separating and assigning features to X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4181b9-eb41-4ec5-ae58-5d0a0e0509a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating & assigning features and target columns to X & y\n",
    "y = original_df['status']\n",
    "X = original_df.drop('status',axis=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f8aeb-8107-4adb-bf51-d1a7b53b903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92866d22-be3c-4674-8f6f-0ad3a3554569",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f0671-9b9f-4d92-883c-dfebd7a7e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the status of the urls' in where legitimate = 1 and phishing = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f0eed-9bd7-4881-b555-7f50aeb805ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the rows to prepare for splitting and training the test set\n",
    "\n",
    "# Shuffling the rows in the dataset so that when splitting the train and test set are equally distributed\n",
    "original_df = original_df.sample(frac=1).reset_index(drop=True)\n",
    "original_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643903f-170b-43fa-8db8-aa808463c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=50, \n",
    "                                                    random_state=78, \n",
    "                                                    stratify=y) \n",
    "# random_state was originally 42\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d56ab-17c7-41a7-bf9d-278d2e7e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the result of StandardScaler\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc50aa9-4b75-43d9-8573-3171aecb0fb3",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280d172-a9bf-4e21-9f68-40a2e16ea609",
   "metadata": {},
   "source": [
    "Random Forest classifier is a type of ensemble learning model combines multiple smaller models into a more robust and accurate model. Random Forest Models use a number of weak learner algorithms (such as decision trees) and combines their output to make a final classification decision. They are very similar to their neural networks counterparts. Random forest models are the most commonly used model because of their robustness and scalability. Both output and feature selection of random forest models are easy to interpret and they can easily handle outliers and nonlinear data.\n",
    "\n",
    "Random forest algorithms are very beneficial because they:\n",
    "- are robust against overfitting as all of those weak learners are trained on different pieces of the data\n",
    "- can be used to rank the importance of input values in a natural way.\n",
    "- can handle thousands of input variables without variable deletion.\n",
    "- are robust to outliers and nonlinear data. \n",
    "- run efficiently using large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff70d5-d4b6-4d48-a652-f204c9b413dd",
   "metadata": {},
   "source": [
    "##### n=128 estimators is the largest value of estimators we would use in a model. To create our random forest classifier model and test the performance, the following code is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ad293-6fd8-4553-b613-fedbdffe2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, \n",
    "                                  random_state=78)\n",
    "\n",
    "# max_depth, min_samples_split, max_features\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253cd01-3119-4058-8aea-9d0284bc7829",
   "metadata": {},
   "source": [
    "## Observation: The test size of the data was separated 50% and it gave us a higher accuracy this way. 98% compared to 96%. See below for the random forest classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d49c-eafe-4430-a428-de4399410614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b8df4-c789-4c6e-bcad-28169a14336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)\n",
    "display.plot()\n",
    "# plt.savefig(\"archive/rfmodel_confusion_matrix_best.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600c920-68f7-4db7-808c-a031b06a19a2",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988cdb8-0be6-435e-93c4-ee119e332d78",
   "metadata": {},
   "source": [
    "The real risk (downside) is when a phishing url is labeled as legitimate. That is the top right quadrant of the image above. This means it has created a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c72c3f-b5f3-440c-86c0-1c49733a7a93",
   "metadata": {},
   "source": [
    "There is also opportunity loss when a legitimate url is labeled as phishing. That is the bottom left quadrant of the image above. This means it has created a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8d900-e199-4535-ad7e-fb31c37309c8",
   "metadata": {},
   "source": [
    "Top left quadrant (or the True Positive) are urls are that actually phishing. Bottom right quadrant (or the True Negative) are urls that actually legitimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0cb31-b83a-43f3-baba-264cbe15198d",
   "metadata": {},
   "source": [
    "### What makes a good Confusion Matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56456d8a-ac75-4032-a7d0-811e64c0ce0c",
   "metadata": {},
   "source": [
    "Good Confusion Matrix = FP < FN.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "This would mean that the user would rather have Legitimate URLs that are accidentally labeled as Phishing (Higher FN) & there would be less Phishing URLs that are accidentally labeled as Legitimate (Lower FP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3551d-b8ea-4be2-8ea5-c06c7e1f0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd62d57-b146-4ff0-b4f0-bf09c981a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature importances from model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# get column names\n",
    "columns = columns\n",
    "\n",
    "# create a dataframe\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': columns,\n",
    "    'importance': importances\n",
    "}) \n",
    "\n",
    "\n",
    "feature_importance = feature_importances_df.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "feature_importance = feature_importances_df.reset_index(drop=True, inplace=True)\n",
    "feature_importance = feature_importances_df.head(10)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5b197-4a5d-4009-b576-85c1d2938d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort all the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e932b3-fd30-4f56-b96b-569c196d4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b1fa0-069b-43cb-bbf9-d86245ca3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the top ten feature-importances_\n",
    "\n",
    "feature_importance.plot.barh(x=\"feature\", y=\"importance\")\n",
    "plt.title(\"Top Ten Feature Importances\")\n",
    "# plt.savefig(\"archive/rf_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b7a2f-7929-4bce-8029-89624ad9a410",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a021a0f-2501-4c68-8ab4-4dfaee1b35e7",
   "metadata": {},
   "source": [
    "Here we test out dataset with a deep learning model to compare with the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a51801-8043-4ae0-8aad-e446bccb1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "# this is done using the classes within the Keras module\n",
    "number_input_features = X_train.shape[1]\n",
    "hidden_nodes_layer1 = (number_input_features * 3)\n",
    "hidden_nodes_layer2 = (number_input_features * 1)\n",
    "hidden_nodes_layer3 = (number_input_features * 0.50)\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "# third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer3,\n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(\n",
    "    units=1, \n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5261ae8-45cb-44d0-9d4a-5f45ac8ffb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fcc98-37cf-441d-9487-c9d3428fdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4b892-bdfb-4441-a989-9a2a288844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd08a9f-3a00-4bd5-b926-86a3fba39925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2595b2-f817-4af8-a0ab-a317d635b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bce0b1-7a28-451d-bdf2-03fd8e7af947",
   "metadata": {},
   "source": [
    "## Model Assessment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5824d-f8f0-4b11-b32e-f85fca28df1e",
   "metadata": {},
   "source": [
    "According to the accuracy metric of the neural network model was able to correctly classify about 98%\n",
    "\n",
    "Loss: 0.11522267115302384, Accuracy: 0.9800000190734863"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d88e23-2088-4a32-8d1f-1d896fc244e3",
   "metadata": {},
   "source": [
    "## Use a Different Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe20115-a1b2-4fbe-91b2-8955604ade25",
   "metadata": {},
   "source": [
    "It is important to use an activation function that matches the complexity of the input data. If we wanted to rank the four most-used activation functions by data complexity and ideal use case, the order would be as follows:\n",
    "\n",
    "- The sigmoid function values are normalized to a probability between 0 and 1, which is ideal for binary classification (like our output classification)\n",
    "- The tanh function can be used for classification or regression, and it expands the range between -1 and 1.\n",
    "- The ReLU function is ideal for looking at positive nonlinear input data for classification or regression.\n",
    "- The Leaky ReLU function is a good alternative for nonlinear input data with many negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36a210-fac9-442c-9119-99349b9e6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "# this is done using the classes within the Keras module\n",
    "number_input_features = X_train.shape[1]\n",
    "hidden_nodes_layer1 =  50\n",
    "hidden_nodes_layer2 = 25\n",
    "hidden_nodes_layer3 = 30\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "# third hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer3,\n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(\n",
    "    units=1, \n",
    "    activation=\"sigmoid\"\n",
    "))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaad397-e692-4d48-aef6-c0cbc5cf397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model together and customize metrics\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model_new = nn_new.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eeccdc-01a5-466b-8cf6-3004408bd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_new_df = pd.DataFrame(fit_model_new.history, index=range(1,len(fit_model_new.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_new_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65e97a-3727-4320-afce-5f98de3ff3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_new_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e060af-5b75-4347-b1ff-2baf705f92ea",
   "metadata": {},
   "source": [
    "## Model Assessment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc757e-1f0a-461f-95e4-36895d2b61be",
   "metadata": {},
   "source": [
    "According to the accuracy metric of the neural network model was only able to correctly classify about 98% (huge improvement from Segment 2 submission ML code)\n",
    "\n",
    "Loss: 0.053311817497014996, Accuracy: 0.9800000190734863"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59431705-81da-4285-b64e-9afc470a4cf4",
   "metadata": {},
   "source": [
    "## Model Performance Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13591cbe-7dbe-4ebe-858c-d12c5689bab3",
   "metadata": {},
   "source": [
    "Both optimized deep learning model and the random forest model were able to predict whether or not a url is phishing with an accuracy rate above 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda028d-6043-4cd8-a0af-eb4c8da7a104",
   "metadata": {},
   "source": [
    "Although they both performed comparably, the implementation and training times were not the same. The random forest classifier was able to train using the large dataset and predict the values within a few seconds, while the deep learning model required more than a few minutes to train on 11430 data points required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75777367-f2df-4f3e-9fda-89dd74ab92ce",
   "metadata": {},
   "source": [
    "The random forest model is able to achieve comparable accuracy on large tabular data with a lot less code and with faster performance. The decision on whether to use random forest classifier versus the deep learning model comes down to preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bef2fd-9d12-42bc-a0ee-461406684331",
   "metadata": {},
   "source": [
    "Since our Phishing Detection dataset is tabular, the Random Forest Classifier is the recommended model based on performance, speed, explainability and simplicity of setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a5d9b-51f3-4806-8cc9-58fe7ad8afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another model to try: Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee243622-069a-4b07-b7a3-a3ce6466401e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b943985-2915-4371-9215-b0aeb3fdd826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
